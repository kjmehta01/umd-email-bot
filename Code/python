nltk_results = ne_chunk(pos_tag(word_tokenize("")))
for nltk_result in nltk_results:
    if type(nltk_result) == Tree:
        name = ''
        for nltk_result_leaf in nltk_result.leaves():
            name += nltk_result_leaf[0] + ' '
        print ('Type: ', nltk_result.label(), 'Name: ', name)
        keywords_list.append(name)

print(keywords_list)


    #body_text=body_text.text
    #body_text = soup.find('section', id='section-body-text')

    #body_text = body_text.text
    #print(body_text.text)
#print(text_list)


"""
second_list = []
for url in url_list:
    single_msg_page = requests.get(url)
    soup = bs4.BeautifulSoup(single_msg_page.text, 'html.parser')
    signature = soup.find_all('umd-signature')
    text_list.append(signature)
    for x in text_list:
        capture_sig = x.find_all('p')
        print(capture_sig)
            #print(signature)
        second_list.append(capture_sig)
    #print(this)

    for signature in url:
        text_list.append(signature)

    for x in signature:
        capture_sig = x.find_all('p')
        print(capture_sig)
        #print(signature)
        text_list.append(capture_sig)

"""
#print(text_list)
#print(second_list)
"""
    sig_text = soup.find_all('div', class_="rich-text")
    if sig_text in signature:
        text_list.append(sig_text)
    else:
        pass

print(text_list)


#print(msg)

def extract_entities(text):
    for sent in nltk.sent_tokenize(text):
        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):
            if hasattr(chunk, 'node'):
                 print(chunk.node, ' '.join(c[0] for c in chunk.leaves()))
    keywords_list.append(c[0] for c in chunk.leaves())
extract_entities(msg)
print(keywords_list)

#for elements in div:
    #title = soup.find('div', class_="feed-item-title")

#for div in divs:
    #print(div.text
